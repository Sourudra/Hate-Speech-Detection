{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vYcP-ZekFnKS"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/hinglish_hate_speech_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qQYKj3WrBVKC"
      },
      "outputs": [],
      "source": [
        "from transformers import TFBertModel\n",
        "\n",
        "# Load the trained model with custom objects\n",
        "model = tf.keras.models.load_model(\n",
        "    model_path,\n",
        "    custom_objects={\"TFBertModel\": TFBertModel}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtEHYZojBlIa",
        "outputId": "100596bc-44a2-4810-c6fa-5ea5e5a4c727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "Prediction: Non-Hate Speech\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the input text\n",
        "input_text = \"example hate speech text\"\n",
        "encoded_input = tokenizer.encode_plus(\n",
        "    input_text,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    max_length=128,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "# Prepare input for the model\n",
        "inputs = {\n",
        "    \"input_ids\": encoded_input[\"input_ids\"],\n",
        "    \"attention_mask\": encoded_input[\"attention_mask\"]\n",
        "}\n",
        "\n",
        "# Make predictions\n",
        "prediction = model.predict(inputs)\n",
        "label = \"Hate Speech\" if prediction[0] > 0.5 else \"Non-Hate Speech\"\n",
        "print(f\"Prediction: {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "YxBgL7OvGhYx",
        "outputId": "a923e179-2226-48fc-f805-5d11c290683a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6b5e304194ea34aae9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://6b5e304194ea34aae9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Function to make predictions\n",
        "def classify_text(input_text):\n",
        "    # Tokenize the input text\n",
        "    encoded_input = tokenizer.encode_plus(\n",
        "        input_text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    # Prepare inputs for the model\n",
        "    inputs = {\n",
        "        \"input_ids\": encoded_input[\"input_ids\"],\n",
        "        \"attention_mask\": encoded_input[\"attention_mask\"]\n",
        "    }\n",
        "\n",
        "    # Make predictions\n",
        "    prediction = model.predict(inputs)\n",
        "    label = \"Hate Speech\" if prediction[0] > 0.5 else \"Non-Hate Speech\"\n",
        "    return label\n",
        "\n",
        "# Define the layout using Blocks\n",
        "with gr.Blocks(css=\".center {text-align: center;}\") as interface:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"<h1 class='center'>Hinglish Hate Speech Detection</h1>\")\n",
        "            gr.Markdown(\"<p class='center'>Type a comment to classify it as 'Hate Speech' or 'Non-Hate Speech'.</p>\")\n",
        "\n",
        "    input_box = gr.Textbox(lines=2, placeholder=\"Enter text here...\")\n",
        "    classify_button = gr.Button(\"Classify\")\n",
        "    output_label = gr.Textbox(label=\"Prediction\", interactive=False)\n",
        "\n",
        "    classify_button.click(classify_text, inputs=input_box, outputs=output_label)\n",
        "\n",
        "# Launch the app\n",
        "interface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
