{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENwqhSahlQ5X",
        "outputId": "c536e199-935c-4b1c-d483-abd9e7d61963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  i am muhajir aur mere lye sab se pehly pakista...   \n",
            "1  doctor sab sahi me ke phd in hate politics wal...   \n",
            "2  poore desh me patel obc me aate hain sirf gujr...   \n",
            "3  ek dil ek jaan akal ke imitihaan kal kp ke rap...   \n",
            "4  i am very sorry to say saaf dil shilpa ke fans...   \n",
            "\n",
            "                                              Tokens  \n",
            "0  [i, am, muhajir, aur, mere, lye, sab, se, pehl...  \n",
            "1  [doctor, sab, sahi, me, ke, phd, in, hate, pol...  \n",
            "2  [poore, desh, me, patel, obc, me, aate, hain, ...  \n",
            "3  [ek, dil, ek, jaan, akal, ke, imitihaan, kal, ...  \n",
            "4  [i, am, very, sorry, to, say, saaf, dil, shilp...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/all_hate_speech.csv\")\n",
        "\n",
        "# Step 1: Tokenization with Regex (Word Splitting)\n",
        "def regex_tokenizer(text):\n",
        "    \"\"\"\n",
        "    Tokenizes text using a regex pattern to extract words.\n",
        "    The pattern considers words with alphabets and handles Hinglish.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r'\\w+', text)  # Extract words (alphanumeric)\n",
        "    return tokens\n",
        "\n",
        "# Step 2: Apply the tokenizer to the 'text' column (correct column name)\n",
        "df['Tokens'] = df['text'].apply(regex_tokenizer)\n",
        "\n",
        "# Display the tokenized result\n",
        "print(df[['text', 'Tokens']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"tokenized_hate_speech.csv\", index=False)"
      ],
      "metadata": {
        "id": "124--VlpmWnL"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}